{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50665876-9882-4f30-8f64-a377581945e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0955d8ee-7346-4fce-b7bc-174fb030e1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_base_preprocess(data_base,test_newin=\"\"):\n",
    "    \n",
    "    '''\n",
    "    功能：预处理data_base格式\n",
    "    1. data_base:读取的VSIM表格\n",
    "    2.test_newin默认空，检测数据的项目号，需要在data_base中删除\n",
    "    '''\n",
    "    \n",
    "    #提取所需要的列\n",
    "    data_base = data_base[[\"NUMBER\",\"NEWIN\",\"NAME\",\"PSS\",\"PLANT\",\"DRAWING NO\",\"EXP\",\"ORD\",\"REQ\",\"S\",\"STATE\",\"LC\",\"UL\",\"LENGTH(MM)\",\"WIDTH(MM)\",\"HEIGHT(MM)\",\"EMB1\",\"FGRP\",\"POS\"]]\n",
    "    #删掉test检测数据\n",
    "    data_base = data_base[data_base[\"NEWIN\"] !=test_newin]\n",
    "    \n",
    "    # 通过NAME拆分INFO TYPE\n",
    "    data_base[\"NAMELIST\"] = data_base[\"NAME\"].str.rsplit(\"-\", n=0)\n",
    "    \n",
    "    data_base[\"TYPE\"] = data_base[\"NAMELIST\"].str[0].str.rsplit(\",\").str[0]\n",
    "    data_base[\"INFO\"] = data_base[\"NAMELIST\"].str[1]\n",
    "    \n",
    "    #剔除UL=0，UL或者LC或者长宽高为空的数据\n",
    "    data_base = data_base.loc[-( (data_base[\"UL\"] == 0) | (data_base[\"UL\"].isna())| (data_base[\"LENGTH(MM)\"].isna() | data_base[\"LC\"].isna()))]\n",
    "    data_base = data_base.loc[-((data_base[\"EXP\"] == \"Y\") | (data_base[\"S\"] == \"N\"))]\n",
    "    \n",
    "    data_base[\"single_volume\"] = data_base[\"LENGTH(MM)\"] * data_base[\"WIDTH(MM)\"]*data_base[\"HEIGHT(MM)\"]/data_base[\"UL\"]\n",
    "    \n",
    "    return data_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4626af32-d69d-4a16-a844-d7537127e5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_preprocess(axa,single_bom,plant):\n",
    "        \n",
    "        test1 = axa[[\"PART NO\",\"PART NAME\",\"PART DESCRIPTION\"]]\n",
    "        test2 = single_bom[[\"Fgrp\",\"Pos \",\"PSS\",\"-Partno-\",\"Description-------\",\"Proj\"]]\n",
    "        test1[[\"PART NAME\",\"PART DESCRIPTION\"]] = test1[[\"PART NAME\",\"PART DESCRIPTION\"]].apply(lambda x : x.apply(lambda y:str(y).strip()))\n",
    "        test2[\"Description-------\"] = test2[\"Description-------\"].apply(lambda y:str(y).strip())\n",
    "        test = pd.merge(test1,test2,how = \"right\",left_on = \"PART NO\",right_on = \"-Partno-\")\n",
    "        \n",
    "        test[\"NAME\"] = test[\"PART NAME\"]+\"-\"+test[\"PART DESCRIPTION\"]\n",
    "        test[\"NAME\"] = test[\"NAME\"].apply(lambda x : str(x).replace(\">\",\"\"))\n",
    "        \n",
    "        test[\"NAMELIST\"] = test[\"NAME\"].str.rsplit(\"-\", n=0)\n",
    "        \n",
    "        test[\"TYPE\"] = test[\"NAMELIST\"].str[0].str.rsplit(\",\").str[0]\n",
    "        test[\"INFO\"] = test[\"NAMELIST\"].str[1]\n",
    "        \n",
    "        test.rename(columns = {\"Fgrp\":\"FGRP\",\"Pos \":\"POS\",\"PART NO\":\"NUMBER\",\"Proj\":\"NEWIN\"},inplace = True)\n",
    "        \n",
    "        test.dropna(subset = [\"NAME\",\"FGRP\",\"POS\",\"NUMBER\"],inplace = True)\n",
    "        \n",
    "        test[\"PLANT\"] = plant\n",
    "        \n",
    "        return test[[\"NUMBER\",\"NAME\",\"PLANT\",\"TYPE\",\"INFO\",\"FGRP\",\"POS\",\"PSS\",\"NEWIN\"]].drop_duplicates().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6953d77-987c-4365-8d97-6dbfd9bcf222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TYPE_process(data_base,mylist):\n",
    "    \n",
    "    '''\n",
    "    删除type中的干扰项\n",
    "    '''\n",
    "    tt = data_base[\"TYPE\"].apply(lambda x: x.replace(\",\", \" \"))\n",
    "    for word in mylist:\n",
    "        tt = tt.apply(lambda x: x.replace(word, \"\"))\n",
    "    data_base[\"TYPE\"] = tt.apply(lambda x: x.strip())\n",
    "    \n",
    "    return data_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708daeb0-d246-48ca-8418-0d17d68b99bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#从database中提取test\n",
    "def test_from_database(df,size=0.02):\n",
    "    \n",
    "    \n",
    "    train,test = train_test_split(df,test_size = size)\n",
    "    \n",
    "    return train.reset_index(drop = True),test.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfda554-71c3-49e7-be88-d3b1f1733f62",
   "metadata": {},
   "source": [
    "以上3个cell说明同流程图"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc1e2ac-8e20-4089-90f9-aded50336693",
   "metadata": {},
   "source": [
    "### 定义betaencoder类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc95878-df11-47ee-8fe1-4f984a8cb24e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BetaEncoder(object):\n",
    "    \n",
    "    def __init__(self, group):\n",
    "        \n",
    "        self.group = group\n",
    "        self.stats = None\n",
    "    \n",
    "    # get counts from df\n",
    "    def fit(self, df, target_col):\n",
    "        # 先验均值\n",
    "        self.prior_mean = np.mean(df[target_col]) \n",
    "        stats           = df[[target_col, self.group]].groupby(self.group)\n",
    "        # count和sum\n",
    "        stats           = stats.agg(['sum', 'count'])[target_col]    \n",
    "        stats.rename(columns={'sum': 'n', 'count': 'N'}, inplace=True)\n",
    "        stats.reset_index(level=0, inplace=True)           \n",
    "        self.stats      = stats\n",
    "    \n",
    "    # extract posterior statistics\n",
    "    def transform(self, df, stat_type, N_min=1):\n",
    "        \n",
    "        df_stats = pd.merge(df[[self.group]], self.stats, how='left')\n",
    "        n        = df_stats['n'].copy()\n",
    "        N        = df_stats['N'].copy()\n",
    "        \n",
    "        # fill in missing\n",
    "        nan_indexs    = np.isnan(n)\n",
    "        n[nan_indexs] = self.prior_mean\n",
    "        N[nan_indexs] = 1.0\n",
    "        \n",
    "        # prior parameters\n",
    "        N_prior     = np.maximum(N_min-N, 0)\n",
    "        alpha_prior = self.prior_mean*N_prior\n",
    "        beta_prior  = (1-self.prior_mean)*N_prior\n",
    "        \n",
    "        # posterior parameters\n",
    "        alpha       =  alpha_prior + n\n",
    "        beta        =  beta_prior  + N-n\n",
    "        \n",
    "        # calculate statistics\n",
    "        if stat_type=='mean':\n",
    "            num = alpha\n",
    "            dem = alpha+beta\n",
    "        \n",
    "        elif stat_type=='mode':\n",
    "            num = alpha-1\n",
    "            dem = alpha+beta-2\n",
    "        \n",
    "        elif stat_type=='median':\n",
    "            num = alpha-1/3\n",
    "            dem = alpha+beta-2/3\n",
    "        \n",
    "        elif stat_type=='var':\n",
    "            num = alpha*beta\n",
    "            dem = (alpha+beta)**2*(alpha+beta+1)\n",
    "        \n",
    "        elif stat_type=='skewness':\n",
    "            num = 2*(beta-alpha)*np.sqrt(alpha+beta+1)\n",
    "            dem = (alpha+beta+2)*np.sqrt(alpha*beta)\n",
    "        \n",
    "        elif stat_type=='kurtosis':\n",
    "            num = 6*(alpha-beta)**2*(alpha+beta+1) - alpha*beta*(alpha+beta+2)\n",
    "            dem = alpha*beta*(alpha+beta+2)*(alpha+beta+3)\n",
    "        \n",
    "        # replace missing\n",
    "        value = num/dem\n",
    "        #value[np.isnan(value)] = np.nanmedian(value)\n",
    "        return value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c5e1407-b4d6-4615-a3c4-a6ddfb95cf81",
   "metadata": {},
   "source": [
    "### 基于beta_target_encoding对特征X进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab87b971-3de5-4293-8393-b5d316a3129c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_encode_X(train,test,cat_cols,N_min):\n",
    "    \n",
    "    feature_cols = []    \n",
    "    target_cols = \"single_volume\"\n",
    "    x_train=pd.DataFrame()\n",
    "    x_test = pd.DataFrame()\n",
    "    \n",
    "    # encode variables\n",
    "    for c in cat_cols:\n",
    "        \n",
    "        # fit encoder\n",
    "        be = BetaEncoder(c)\n",
    "        be.fit(train, 'single_volume')\n",
    "        \n",
    "        # mean\n",
    "        feature_name = f'{c}_mean'\n",
    "        x_train[feature_name] = be.transform(train, 'mean', N_min)\n",
    "        x_test[feature_name]  = be.transform(test,  'mean', N_min)\n",
    "        feature_cols.append(feature_name)\n",
    "        \n",
    "        # mode\n",
    "        feature_name = f'{c}_mode'\n",
    "        x_train[feature_name] = be.transform(train, 'mode', N_min)\n",
    "        x_test[feature_name]  = be.transform(test,  'mode', N_min)\n",
    "        feature_cols.append(feature_name)\n",
    "        \n",
    "        # median\n",
    "        feature_name = f'{c}_median'\n",
    "        x_train[feature_name] = be.transform(train, 'median', N_min)\n",
    "        x_test[feature_name]  = be.transform(test,  'median', N_min)\n",
    "        feature_cols.append(feature_name)    \n",
    "        \n",
    "        # var\n",
    "        feature_name = f'{c}_var'\n",
    "        x_train[feature_name] = be.transform(train, 'var', N_min)\n",
    "        x_test[feature_name]  = be.transform(test,  'var', N_min)\n",
    "        feature_cols.append(feature_name)        \n",
    "        \n",
    "        # skewness\n",
    "        feature_name = f'{c}_skewness'\n",
    "        x_train[feature_name] = be.transform(train, 'skewness', N_min)\n",
    "        x_test[feature_name]  = be.transform(test,  'skewness', N_min)\n",
    "        feature_cols.append(feature_name)    \n",
    "        \n",
    "        # kurtosis\n",
    "        feature_name = f'{c}_kurtosis'\n",
    "        x_train[feature_name] = be.transform(train, 'kurtosis', N_min)\n",
    "        x_test[feature_name]  = be.transform(test,  'kurtosis', N_min)\n",
    "        feature_cols.append(feature_name)  \n",
    "\n",
    "        x_train = x_train.dropna(axis=1)\n",
    "        x_test = x_test.dropna(axis=1)\n",
    "        \n",
    "    return x_train,x_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c83c49-886e-43d6-a7a4-fec8400fe68f",
   "metadata": {},
   "source": [
    "### 基于labelencoder对目标Y值进行编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a27578-0777-45a6-9a47-9771927551ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_encode_Y(train):\n",
    "    \n",
    "\n",
    "    enc=preprocessing.LabelEncoder()\n",
    "    #y_encoded=pd.Series(enc.fit_transform(mydata[\"single_volume\"]),index = mydata.index ) #训练LabelEncoder\n",
    "    \n",
    "    y_encoded = enc.fit_transform(train[\"single_volume\"])\n",
    "    \n",
    "    y_train = enc.transform(train[\"single_volume\"])\n",
    "    \n",
    "    return y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d81b24-674e-4d76-afc3-0ab454182768",
   "metadata": {},
   "source": [
    "### 训练数据集，此处为随机森林模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c19ed4-cf6d-498e-a025-b640b14e1ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit(x_train,y_train):\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=15,random_state=0,max_depth=30)  # 实例化模型RandomForestClassifier\n",
    "    print(\"model is fitting\")\n",
    "    model.fit(x_train,y_train)\n",
    "    print(\"Done\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc585062-4303-4635-830e-87e6ef22043a",
   "metadata": {},
   "source": [
    "### 预测测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34bbd0-7c66-45b5-a8a7-937d07892b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(x_test,y_train,train):\n",
    "    \n",
    "    \n",
    "    pre = model.predict(x_test)\n",
    "\n",
    "    \n",
    "    pair = dict(zip(y_train,train[\"single_volume\"]))\n",
    "    \n",
    "    predict_volume = [pair[i] for i in model.predict(x_test)]\n",
    "    \n",
    "    return pre,predict_volume,pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243dbcb7-392b-45d6-af4f-aee3d555f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    #读取VSIM表格\n",
    "    data_base = pd.read_excel(r\"C:\\Users\\zli242\\Desktop\\VOLVO\\0815\\data base.xlsx\")\n",
    "    axa = pd.read_excel(r\"C:\\Users\\zli242\\Desktop\\VOLVO\\packaging final_version\\AXA P519.xlsx\",header =4 ).iloc[4:,:]\n",
    "    single_bom = pd.read_excel(r\"C:\\Users\\zli242\\Desktop\\VOLVO\\packaging final_version\\Single bom.xlsx\",header = 3).iloc[3:,:]\n",
    "    plant = \"CHN03\"\n",
    "    #清洗表格，删除样本数据的项目号\n",
    "    data_base_processed = data_base_preprocess(data_base,test_newin = \"\")\n",
    "    #清洗test\n",
    "    test_processed = test_preprocess(axa, single_bom, plant)\n",
    "    # 删除样本数据和database的干扰项\n",
    "    mylist = [\n",
    "        \"UNCOLORED\",\n",
    "        \"KSOCOLOR\",\n",
    "        \"CKDCOLOR\",\n",
    "        \"CHARCOAL\",\n",
    "        \"OFFBLACK\",\n",
    "        \"CHARCOAL SOLID\",\n",
    "        \"DAWN\",\n",
    "        \"BRIGHT\",\n",
    "        \"CHROME\",\n",
    "        \"GRAINED\",\n",
    "        \"PAINTED\",\n",
    "        \"CARDAMOM\",\n",
    "        \"BLACK\",\n",
    "        \"WHITE\",\n",
    "        \"WOOD\",\n",
    "        \"LIGHT ASH\",\n",
    "        \"LIGHT GUI\",\n",
    "        \"METAL\",\n",
    "        \"LUMIERE\",\n",
    "        \"FOG MELANGE\",\n",
    "    ]\n",
    "    \n",
    "    train = TYPE_process(data_base_processed,mylist)\n",
    "    test = TYPE_process(test_processed,mylist)\n",
    "    '''\n",
    "    #从data_base中提取test,test\n",
    "    train,test = test_from_database(data_base_final,size=0.05)\n",
    "    '''\n",
    "    \n",
    "    #把data_base进行编码\n",
    "    cat_cols = ['NAME',\n",
    "     \"NUMBER\",\n",
    "     \"NEWIN\",\n",
    "     'PLANT',\n",
    "     'TYPE',\n",
    "     'INFO',\n",
    "     \"POS\",\n",
    "     \"PSS\",\n",
    "     \"FGRP\"]\n",
    "    N_min =5\n",
    "    \n",
    "    x_train,x_test = to_encode_X(train,test,cat_cols,N_min)\n",
    "    y_train = to_encode_Y(train)\n",
    "    \n",
    "    #进行模型训练\n",
    "    model= model_fit(x_train,y_train) \n",
    "    #检测\n",
    "    pre,predict_volume,pair = model_predict(x_test, y_train, train)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12667020-4f9f-487c-8b56-fe555fca8a49",
   "metadata": {},
   "source": [
    "### 若test为从data_base中随意挑选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d508a5d7-dac0-43c9-8047-937a9abda244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#完全一致\n",
    "\n",
    "sum([pair[i] for i in pre]==test[\"single_volume\"])/len(test)\n",
    "\n",
    "\n",
    "#0.3 1061 \n",
    "sum((abs([pair[i] for i in pre]- test[\"single_volume\"])/test[\"single_volume\"])<0.3)/len(test)\n",
    "\n",
    "#0.2 1038 \n",
    "sum((abs([pair[i] for i in pre]- test[\"single_volume\"])/ test[\"single_volume\"])<0.2)/len(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
